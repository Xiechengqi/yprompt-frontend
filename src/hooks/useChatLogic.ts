import { useEffect, useMemo, useCallback } from 'react'
import { usePromptStore } from '@/stores/promptStore'
import { useProviderStore } from '@/stores/providerStore'
import { useNotificationStore } from '@/stores/notificationStore'
import { AIGuideService } from '@/services/aiGuideService'
import { AIService } from '@/services/aiService'
import { PromptGeneratorService } from '@/services/promptGeneratorService'
import { getPromptGeneratorConfig } from '@/config/promptGenerator'
import { cleanAIResponse, checkAIDecision } from '@/utils/aiResponseUtils'
import type { useChatMessages } from './useChatMessages'
import type { useChatModel } from './useChatModel'
import type { useChatInput } from './useChatInput'
import type { useChatAttachments } from './useChatAttachments'
import type { useChatQuickReplies } from './useChatQuickReplies'

interface UseChatLogicParams {
  chatMessages: ReturnType<typeof useChatMessages>
  chatModel: ReturnType<typeof useChatModel>
  chatInput: ReturnType<typeof useChatInput>
  chatAttachments: ReturnType<typeof useChatAttachments>
  chatQuickReplies: ReturnType<typeof useChatQuickReplies>
}

export function useChatLogic({
  chatMessages,
  chatModel,
  chatInput,
  chatAttachments,
  chatQuickReplies
}: UseChatLogicParams) {
  const promptStore = usePromptStore()
  const providerStore = useProviderStore()
  const notificationStore = useNotificationStore()
  const config = getPromptGeneratorConfig()
  const aiGuideService = AIGuideService.getInstance()

  const isAbortError = useCallback((error: unknown) => {
    return (error instanceof DOMException && error.name === 'AbortError') || (error as any)?.name === 'AbortError'
  }, [])

  const chatContainerMaxHeight = useMemo(() => {
    let baseCalculation = 345
    let attachmentExtraHeight = 0
    if (chatAttachments.currentAttachments.length > 0) {
      attachmentExtraHeight = 115
    }
    const totalReduction = baseCalculation + attachmentExtraHeight
    return `calc(100vh - ${totalReduction}px)`
  }, [chatAttachments.currentAttachments.length])

  useEffect(() => {
    chatMessages.scrollToBottom()
  }, [promptStore.chatMessages.length, chatMessages])

  useEffect(() => {
    chatMessages.scrollToBottom()
  }, [promptStore.isTyping, chatMessages.scrollToBottom])

  const initializeChat = useCallback(async () => {
    const state = promptStore.getState()
    if (state.chatMessages.length === 0 && !state.isInitialized) {
      promptStore.setState({ isInitialized: true })
      await chatMessages.simulateTyping(config.welcomeMessage, false)
    }
  }, [promptStore, chatMessages, config])

  const clearChat = useCallback(() => {
    promptStore.clearChat()
    chatQuickReplies.setShowQuickReplies(false)
    chatAttachments.clearAttachments()
    
    setTimeout(async () => {
      await chatMessages.simulateTyping(config.welcomeMessage, false)
      promptStore.setState({ isInitialized: true })
    }, 500)
  }, [promptStore, chatQuickReplies, chatAttachments, chatMessages, config])

  const generatePrompt = useCallback(async (provider: any, modelId: string) => {
    try {
      promptStore.clearProgressMessages()

      const validMessages = promptStore.getValidMessages()
      const conversationHistory = validMessages.map(msg => ({
        type: msg.type,
        content: msg.content,
        attachments: msg.attachments || []
      }))
      
      promptStore.setIsGenerating(true)
      promptStore.setCurrentExecutionStep('report')
      promptStore.addOrUpdateProgressMessage('ğŸ”„ æ­£åœ¨åŸºäºå¯¹è¯ç”Ÿæˆéœ€æ±‚æŠ¥å‘Š...', 'progress')
      
      const initState = promptStore.getState()
      promptStore.setState({ promptData: { ...initState.promptData, requirementReport: '' } })
      
      const onReportStreamUpdate = (chunk: string) => {
        const state = promptStore.getState()
        promptStore.promptData.requirementReport = (state.promptData.requirementReport || '') + chunk
        // è§¦å‘çŠ¶æ€æ›´æ–°
        promptStore.setState({ promptData: { ...state.promptData, requirementReport: state.promptData.requirementReport + chunk } })
      }
      
      const requirementReport = await aiGuideService.generateRequirementReportFromConversation(
        conversationHistory,
        provider,
        modelId,
        providerStore.streamMode ? onReportStreamUpdate : undefined
      )

      if (!providerStore.streamMode) {
        const state = promptStore.getState()
        promptStore.setState({ promptData: { ...state.promptData, requirementReport } })
      }
      promptStore.setShowPreview(true)
      
      const state = promptStore.getState()
      if (state.isAutoMode) {
        promptStore.addOrUpdateProgressMessage('âœ… éœ€æ±‚æŠ¥å‘Šå·²ç”Ÿæˆï¼æ­£åœ¨è‡ªåŠ¨æ‰§è¡Œå®Œæ•´çš„æç¤ºè¯ç”Ÿæˆæµç¨‹...', 'progress')
        
        const promptGeneratorService = PromptGeneratorService.getInstance()
        
        promptStore.setCurrentExecutionStep('thinking')
        promptStore.addOrUpdateProgressMessage('ğŸ”„ æ­¥éª¤ 1/4: æ­£åœ¨åˆ†æéœ€æ±‚å¹¶ç”Ÿæˆå…³é”®æŒ‡ä»¤...', 'progress')
        
        let step1Content = ''
        const onStep1Update = (chunk: string) => {
          step1Content += chunk
          const points = step1Content.split('\n').map(s => s.replace(/^[*-]\s*/, '').trim()).filter(Boolean)
          if (points.length > 0) {
            const step1State = promptStore.getState()
            promptStore.setState({ promptData: { ...step1State.promptData, thinkingPoints: points } })
          }
        }
        
        const step1State = promptStore.getState()
        const thinkingPoints = await promptGeneratorService.getSystemPromptThinkingPoints(
          step1State.promptData.requirementReport || requirementReport,
          modelId,
          'zh',
          [],
          provider,
          providerStore.streamMode ? onStep1Update : undefined
        )
        
        if (!providerStore.streamMode) {
          const afterStep1State = promptStore.getState()
          promptStore.setState({ promptData: { ...afterStep1State.promptData, thinkingPoints } })
        }
        
        promptStore.setCurrentExecutionStep('initial')
        promptStore.addOrUpdateProgressMessage('ğŸ”„ æ­¥éª¤ 2/4: æ­£åœ¨åŸºäºå…³é”®æŒ‡ä»¤ç”Ÿæˆåˆå§‹æç¤ºè¯...', 'progress')
        
        const step2State = promptStore.getState()
        promptStore.setState({ promptData: { ...step2State.promptData, initialPrompt: '' } })
        const onStep2Update = (chunk: string) => {
          const state = promptStore.getState()
          const newInitialPrompt = (state.promptData.initialPrompt || '') + chunk
          promptStore.setState({ promptData: { ...state.promptData, initialPrompt: newInitialPrompt } })
        }
        
        const step2StateForPrompt = promptStore.getState()
        const initialPrompt = await promptGeneratorService.generateSystemPrompt(
          step2StateForPrompt.promptData.requirementReport || requirementReport,
          modelId,
          'zh',
          [],
          step2StateForPrompt.promptData.thinkingPoints || thinkingPoints,
          provider,
          providerStore.streamMode ? onStep2Update : undefined
        )
        
        if (!providerStore.streamMode) {
          const afterStep2State = promptStore.getState()
          promptStore.setState({ promptData: { ...afterStep2State.promptData, initialPrompt } })
        }
        
        promptStore.setCurrentExecutionStep('advice')
        promptStore.addOrUpdateProgressMessage('ğŸ”„ æ­¥éª¤ 3/4: æ­£åœ¨åˆ†ææç¤ºè¯å¹¶ç”Ÿæˆä¼˜åŒ–å»ºè®®...', 'progress')
        
        let step3Content = ''
        const onStep3Update = (chunk: string) => {
          step3Content += chunk
          const adviceList = step3Content.split('\n').map(s => s.replace(/^[*-]\s*/, '').trim()).filter(Boolean)
          if (adviceList.length > 0) {
            const step3State = promptStore.getState()
            promptStore.setState({ promptData: { ...step3State.promptData, advice: adviceList } })
          }
        }
        
        const step3StateForAdvice = promptStore.getState()
        const advice = await promptGeneratorService.getOptimizationAdvice(
          step3StateForAdvice.promptData.initialPrompt || initialPrompt,
          'system',
          modelId,
          'zh',
          [],
          provider,
          providerStore.streamMode ? onStep3Update : undefined
        )
        
        if (!providerStore.streamMode) {
          const afterStep3State = promptStore.getState()
          promptStore.setState({ promptData: { ...afterStep3State.promptData, advice } })
        }
        
        promptStore.setCurrentExecutionStep('final')
        promptStore.addOrUpdateProgressMessage('ğŸ”„ æ­¥éª¤ 4/4: æ­£åœ¨åº”ç”¨ä¼˜åŒ–å»ºè®®ï¼Œç”Ÿæˆæœ€ç»ˆæç¤ºè¯...', 'progress')
        
        const step4InitState = promptStore.getState()
        promptStore.setState({ promptData: { ...step4InitState.promptData, generatedPrompt: '' } })
        const onStep4Update = (chunk: string) => {
          const step4State = promptStore.getState()
          const currentPrompt = typeof step4State.promptData.generatedPrompt === 'string' 
            ? step4State.promptData.generatedPrompt 
            : ('zh' in step4State.promptData.generatedPrompt ? step4State.promptData.generatedPrompt.zh : '')
          const newPrompt = currentPrompt + chunk
          promptStore.setState({ promptData: { ...step4State.promptData, generatedPrompt: newPrompt } })
        }
        
        const step4StateForFinal = promptStore.getState()
        const finalPrompt = await promptGeneratorService.applyOptimizationAdvice(
          step4StateForFinal.promptData.initialPrompt || initialPrompt,
          step4StateForFinal.promptData.advice || advice,
          'system',
          modelId,
          'zh',
          [],
          provider,
          providerStore.streamMode ? onStep4Update : undefined
        )
        
        if (!providerStore.streamMode) {
          const afterStep4State = promptStore.getState()
          promptStore.setState({ promptData: { ...afterStep4State.promptData, generatedPrompt: finalPrompt } })
        }
        promptStore.addOrUpdateProgressMessage('âœ… å·²ä¸ºæ‚¨ç”Ÿæˆé«˜è´¨é‡çš„AIæç¤ºè¯ï¼å³ä¾§å¯æŸ¥çœ‹å®Œæ•´çš„ç”Ÿæˆè¿‡ç¨‹å’Œæœ€ç»ˆç»“æœã€‚', 'progress')
        
      } else {
        promptStore.addOrUpdateProgressMessage('âœ… éœ€æ±‚æŠ¥å‘Šå·²ç”Ÿæˆï¼è¯·åœ¨å³ä¾§é¢„è§ˆé¢æ¿ä¸­æŸ¥çœ‹ï¼Œæ‚¨å¯ä»¥æ‰‹åŠ¨æ‰§è¡Œæ¯ä¸ªæ­¥éª¤ã€‚', 'progress')
      }
      
      promptStore.isGenerating = false
      promptStore.currentExecutionStep = null
      
    } catch (error: unknown) {
      promptStore.setIsGenerating(false)
      promptStore.setCurrentExecutionStep(null)

      if (isAbortError(error)) {
        notificationStore.info('ç”Ÿæˆå·²ä¸­æ–­')
        return
      }

      const errorMessage = error instanceof Error ? error.message : String(error)
      notificationStore.error(`æç¤ºè¯ç”Ÿæˆå¤±è´¥: ${errorMessage}ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®åé‡è¯•`)
    }
  }, [promptStore, providerStore, aiGuideService, notificationStore, isAbortError])

  const sendMessage = useCallback(async (userInput: string) => {
    if (!userInput.trim()) {
      if (chatAttachments.currentAttachments.length > 0) {
        notificationStore.warning('è¯·è¾“å…¥æ¶ˆæ¯å†…å®¹ï¼Œä¸èƒ½åªå‘é€é™„ä»¶')
      }
      return
    }
    
    // ç¡®ä¿ providerStore å·²åˆå§‹åŒ–
    if (!providerStore.isInitialized) {
      console.log('[useChatLogic] providerStore æœªåˆå§‹åŒ–ï¼Œå°è¯•åˆå§‹åŒ–...')
      try {
        await providerStore.initialize()
        // ç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿çŠ¶æ€å·²æ›´æ–°
        await new Promise(resolve => setTimeout(resolve, 100))
      } catch (error) {
        console.error('[useChatLogic] åˆå§‹åŒ– providerStore å¤±è´¥:', error)
        notificationStore.error('æ— æ³•åŠ è½½AIæ¨¡å‹é…ç½®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥')
        return
      }
    }

    // å¦‚æœåˆå§‹åŒ–åä»ç„¶æ²¡æœ‰é€‰æ‹©ï¼Œå°è¯•ä» store è·å–æœ€æ–°çš„çŠ¶æ€
    const storeState = useProviderStore.getState()
    if (!storeState.selectedProviderId && storeState.enabledProviders.length > 0) {
      const firstProvider = storeState.enabledProviders[0]
      console.log('[useChatLogic] æ£€æµ‹åˆ°æ²¡æœ‰é€‰æ‹©ï¼Œè‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªæä¾›å•†:', firstProvider.name)
      storeState.setSelectedProvider(firstProvider.id)
      if (firstProvider.models.length > 0) {
        storeState.setSelectedModel(firstProvider.models[0].id)
      }
      // ç­‰å¾…çŠ¶æ€æ›´æ–°
      await new Promise(resolve => setTimeout(resolve, 50))
    }

    const { provider, model } = chatModel.getCurrentChatModel()

    console.log('[useChatLogic] sendMessage æ£€æŸ¥æ¨¡å‹é…ç½®:', {
      provider,
      model,
      hasProvider: !!provider,
      hasModel: !!model,
      providerName: provider?.name,
      modelName: model?.name,
      isInitialized: providerStore.isInitialized,
      providersCount: providerStore.providers.length,
      selectedProviderId: providerStore.selectedProviderId,
      selectedModelId: providerStore.selectedModelId
    })

    if (!provider || !model) {
      // æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
      const providerStoreState = {
        selectedProviderId: providerStore.selectedProviderId,
        selectedModelId: providerStore.selectedModelId,
        providersCount: providerStore.providers.length,
        isInitialized: providerStore.isInitialized,
        enabledProvidersCount: providerStore.enabledProviders.length
      }

      console.error('[useChatLogic] æ¨¡å‹é…ç½®æ£€æµ‹å¤±è´¥:', {
        providerStoreState,
        currentProvider: providerStore.currentProvider,
        currentModel: providerStore.currentModel,
        enabledProviders: providerStore.enabledProviders.map(p => ({ id: p.id, name: p.name, modelsCount: p.models.length }))
      })

      // å¦‚æœ providers å·²åŠ è½½ä½†æ²¡æœ‰é€‰æ‹©ï¼Œæç¤ºç”¨æˆ·é€‰æ‹©
      if (providerStore.isInitialized && providerStore.enabledProviders.length > 0) {
        notificationStore.warning('è¯·å…ˆåœ¨å¯¼èˆªæ é€‰æ‹©AIæ¨¡å‹ï¼Œæˆ–åœ¨å³ä¸Šè§’è®¾ç½®ä¸­é…ç½®AIæ¨¡å‹å’ŒAPIå¯†é’¥')
      } else if (!providerStore.isInitialized) {
        notificationStore.error('AIæ¨¡å‹é…ç½®åŠ è½½ä¸­ï¼Œè¯·ç¨å€™å†è¯•')
      } else {
        notificationStore.warning('è¯·å…ˆåœ¨å³ä¸Šè§’è®¾ç½®ä¸­é…ç½®AIæ¨¡å‹å’ŒAPIå¯†é’¥')
      }
      return
    }

    promptStore.clearProgressMessages()

    const currentInput = userInput
    const attachments = [...chatAttachments.currentAttachments]
    
    const isForceGenerate = chatQuickReplies.checkForceGenerate(currentInput)
    
    promptStore.addMessage('user', currentInput, attachments)
    
    chatInput.clearInput()
    chatAttachments.clearAttachments()
    chatQuickReplies.setShowQuickReplies(false)
    
    if (isForceGenerate) {
      await chatMessages.simulateTyping('å¥½çš„,æˆ‘å°†ç«‹å³ä¸ºæ‚¨ç”Ÿæˆéœ€æ±‚æŠ¥å‘Šã€‚', false)

      setTimeout(async () => {
        const globalProvider = providerStore.currentProvider
        const globalModel = providerStore.currentModel
        if (globalProvider && globalModel) {
          await generatePrompt(globalProvider, globalModel.id)
        }
      }, 800)
      return
    }

    promptStore.setIsTyping(true)
    promptStore.setIsGenerating(true)

    try {
      const useStreamMode = chatModel.isStreamMode
      
      if (useStreamMode) {
        const aiService = AIService.getInstance()
        
        let streamingContent = ''
        let messageIndex = -1
        
        aiService.setStreamUpdateCallback((chunk: string) => {
          if (messageIndex === -1) {
            messageIndex = chatMessages.startStreamingMessage()
          }
          streamingContent += chunk
          const cleanContent = cleanAIResponse(streamingContent)
          chatMessages.updateStreamingMessage(messageIndex, cleanContent)
          chatMessages.scrollToBottom()
        })
        
        const validMessages = promptStore.getValidMessages()
        const conversationHistory = validMessages.map(msg => ({
          type: msg.type,
          content: msg.content,
          attachments: msg.attachments || []
        }))
        
        const aiResponse = await aiGuideService.generateSimpleResponse(
          '',
          conversationHistory,
          provider,
          model.id,
          useStreamMode
        )

        aiService.clearStreamUpdateCallback()

        if (useStreamMode && messageIndex === -1) {
          messageIndex = chatMessages.startStreamingMessage()
          const cleanContent = cleanAIResponse(aiResponse)
          chatMessages.updateStreamingMessage(messageIndex, cleanContent)
        } else if (useStreamMode && streamingContent.trim() === '') {
          const cleanContent = cleanAIResponse(aiResponse)
          chatMessages.updateStreamingMessage(messageIndex, cleanContent)
        }

        const shouldEndConversation = checkAIDecision(aiResponse)
        
        promptStore.setIsTyping(false)
        promptStore.setIsGenerating(false)
        
        if (shouldEndConversation || aiResponse.includes('åŸºäºæˆ‘ä»¬çš„å¯¹è¯ï¼Œæˆ‘ç°åœ¨ä¸ºæ‚¨ç”Ÿæˆéœ€æ±‚æŠ¥å‘Šï¼š')) {
          setTimeout(async () => {
            const globalProvider = providerStore.currentProvider
            const globalModel = providerStore.currentModel
            if (globalProvider && globalModel) {
              await generatePrompt(globalProvider, globalModel.id)
            }
          }, 800)
        }
      } else {
        const validMessages = promptStore.getValidMessages()
        const conversationHistory = validMessages.map(msg => ({
          type: msg.type,
          content: msg.content,
          attachments: msg.attachments || []
        }))
        const aiResponse = await aiGuideService.generateSimpleResponse(
          '',
          conversationHistory,
          provider,
          model.id,
          useStreamMode
        )

        const shouldEndConversation = checkAIDecision(aiResponse)
        
        if (shouldEndConversation || aiResponse.includes('åŸºäºæˆ‘ä»¬çš„å¯¹è¯ï¼Œæˆ‘ç°åœ¨ä¸ºæ‚¨ç”Ÿæˆéœ€æ±‚æŠ¥å‘Šï¼š')) {
          const cleanResponse = cleanAIResponse(aiResponse)
          await chatMessages.simulateTyping(cleanResponse, false)

          promptStore.setIsGenerating(false)

          setTimeout(async () => {
            const globalProvider = providerStore.currentProvider
            const globalModel = providerStore.currentModel
            if (globalProvider && globalModel) {
              await generatePrompt(globalProvider, globalModel.id)
            }
          }, 800)
        } else {
          const cleanResponse = cleanAIResponse(aiResponse)
          await chatMessages.simulateTyping(cleanResponse, false)
          
          promptStore.isGenerating = false
        }
      }
      } catch (error: unknown) {
      promptStore.setIsTyping(false)
      promptStore.setIsGenerating(false)

      if (isAbortError(error)) {
        if (chatModel.isStreamMode) {
          const aiService = AIService.getInstance()
          aiService.clearStreamUpdateCallback()
        }
        return
      }

      const errorMessage = error instanceof Error ? error.message : String(error)
      notificationStore.error(`å‘ç”Ÿé”™è¯¯: ${errorMessage}`)

      if (chatModel.isStreamMode) {
        const aiService = AIService.getInstance()
        aiService.clearStreamUpdateCallback()
      }
    }
  }, [
    chatAttachments,
    chatModel,
    chatInput,
    chatQuickReplies,
    chatMessages,
    promptStore,
    providerStore,
    notificationStore,
    aiGuideService,
    generatePrompt,
    isAbortError
  ])

  const regenerateMessage = useCallback(async (messageId: string, messageIndex: number, provider: any, model: any) => {
    const message = promptStore.chatMessages.find(msg => msg.id === messageId)
    if (!message || message.type !== 'ai') {
      return
    }

    if (!provider || !model) {
      notificationStore.warning('è¯·å…ˆåœ¨å³ä¸Šè§’è®¾ç½®ä¸­é…ç½®AIæ¨¡å‹å’ŒAPIå¯†é’¥')
      return
    }

    try {
      promptStore.clearProgressMessages()
      
      const contextMessages = promptStore.getValidMessages().slice(0, messageIndex)
      const conversationHistory = contextMessages.map(msg => ({
        type: msg.type,
        content: msg.content,
        attachments: msg.attachments || []
      }))
      
      promptStore.setIsTyping(true)
      
      if (chatModel.isStreamMode) {
        const aiService = AIService.getInstance()
        
        let streamingContent = ''
        
        aiService.setStreamUpdateCallback((chunk: string) => {
          streamingContent += chunk
          const cleanContent = cleanAIResponse(streamingContent)
          promptStore.updateMessage(messageId, cleanContent)
          chatMessages.scrollToBottom()
        })
        
        const aiResponse = await aiGuideService.generateSimpleResponse(
          '',
          conversationHistory,
          provider,
          model.id,
          true
        )

        aiService.clearStreamUpdateCallback()
        
        const finalContent = cleanAIResponse(aiResponse)
        promptStore.updateMessage(messageId, finalContent)
        
      } else {
        const aiResponse = await aiGuideService.generateSimpleResponse(
          '',
          conversationHistory,
          provider,
          model.id,
          false
        )
        
        const cleanResponse = cleanAIResponse(aiResponse)
        promptStore.updateMessage(messageId, cleanResponse)
      }
      
      promptStore.setIsTyping(false)
      notificationStore.success('æ¶ˆæ¯å·²é‡æ–°ç”Ÿæˆ')
      
    } catch (error: unknown) {
      promptStore.setIsTyping(false)

      if (isAbortError(error)) {
        if (chatModel.isStreamMode) {
          const aiService = AIService.getInstance()
          aiService.clearStreamUpdateCallback()
        }
        return
      }

      const errorMessage = error instanceof Error ? error.message : String(error)
      notificationStore.error(`é‡æ–°ç”Ÿæˆå¤±è´¥: ${errorMessage}`)

      if (chatModel.isStreamMode) {
        const aiService = AIService.getInstance()
        aiService.clearStreamUpdateCallback()
      }
    }
  }, [promptStore, chatModel, notificationStore, aiGuideService, chatMessages, isAbortError])

  const resendUserMessage = useCallback(async (messageId: string, messageIndex: number, provider: any, model: any) => {
    const message = promptStore.chatMessages.find(msg => msg.id === messageId)
    if (!message || message.type !== 'user') {
      return
    }

    if (!provider || !model) {
      notificationStore.warning('è¯·å…ˆåœ¨å³ä¸Šè§’è®¾ç½®ä¸­é…ç½®AIæ¨¡å‹å’ŒAPIå¯†é’¥')
      return
    }

    try {
      promptStore.clearProgressMessages()
      
      if (messageIndex !== -1) {
        for (let i = messageIndex + 1; i < promptStore.chatMessages.length; i++) {
          const msg = promptStore.chatMessages[i]
          if (msg && !msg.isProgress) {
            promptStore.deleteMessage(msg.id!)
          }
        }
      }

      promptStore.setIsTyping(true)

      const useStreamMode = chatModel.isStreamMode
      
      if (useStreamMode) {
        const aiService = AIService.getInstance()
        
        let streamingContent = ''
        let msgIndex = -1
        
        aiService.setStreamUpdateCallback((chunk: string) => {
          if (msgIndex === -1) {
            msgIndex = chatMessages.startStreamingMessage()
          }
          streamingContent += chunk
          const cleanContent = cleanAIResponse(streamingContent)
          chatMessages.updateStreamingMessage(msgIndex, cleanContent)
          chatMessages.scrollToBottom()
        })
        
        const validMessages = promptStore.getValidMessages()
        const conversationHistory = validMessages.map(msg => ({
          type: msg.type,
          content: msg.content,
          attachments: msg.attachments || []
        }))
        const aiResponse = await aiGuideService.generateSimpleResponse(
          '',
          conversationHistory,
          provider,
          model.id,
          useStreamMode
        )

        aiService.clearStreamUpdateCallback()

        const shouldEndConversation = checkAIDecision(aiResponse)
        
        promptStore.setIsTyping(false)
        
        if (shouldEndConversation || aiResponse.includes('åŸºäºæˆ‘ä»¬çš„å¯¹è¯ï¼Œæˆ‘ç°åœ¨ä¸ºæ‚¨ç”Ÿæˆéœ€æ±‚æŠ¥å‘Šï¼š')) {
          setTimeout(async () => {
            const globalProvider = providerStore.currentProvider
            const globalModel = providerStore.currentModel
            if (globalProvider && globalModel) {
              await generatePrompt(globalProvider, globalModel.id)
            }
          }, 800)
        }
      } else {
        const validMessages = promptStore.getValidMessages()
        const conversationHistory = validMessages.map(msg => ({
          type: msg.type,
          content: msg.content,
          attachments: msg.attachments || []
        }))
        const aiResponse = await aiGuideService.generateSimpleResponse(
          '',
          conversationHistory,
          provider,
          model.id,
          useStreamMode
        )

        const shouldEndConversation = checkAIDecision(aiResponse)
        
        if (shouldEndConversation || aiResponse.includes('åŸºäºæˆ‘ä»¬çš„å¯¹è¯ï¼Œæˆ‘ç°åœ¨ä¸ºæ‚¨ç”Ÿæˆéœ€æ±‚æŠ¥å‘Šï¼š')) {
          const cleanResponse = cleanAIResponse(aiResponse)
          await chatMessages.simulateTyping(cleanResponse, false)

          promptStore.setIsTyping(false)

          setTimeout(async () => {
            const globalProvider = providerStore.currentProvider
            const globalModel = providerStore.currentModel
            if (globalProvider && globalModel) {
              await generatePrompt(globalProvider, globalModel.id)
            }
          }, 800)
        } else {
          const cleanResponse = cleanAIResponse(aiResponse)
          await chatMessages.simulateTyping(cleanResponse, false)
          
          promptStore.setIsTyping(false)
        }
      }
    } catch (error: unknown) {
      promptStore.setIsTyping(false)
      promptStore.setIsGenerating(false)

      if (isAbortError(error)) {
        if (chatModel.isStreamMode) {
          const aiService = AIService.getInstance()
          aiService.clearStreamUpdateCallback()
        }
        return
      }

      const errorMessage = error instanceof Error ? error.message : String(error)
      notificationStore.error(`é‡æ–°å‘é€å¤±è´¥: ${errorMessage}`)

      if (chatModel.isStreamMode) {
        const aiService = AIService.getInstance()
        aiService.clearStreamUpdateCallback()
      }
    }
  }, [
    promptStore,
    chatModel,
    chatMessages,
    providerStore,
    notificationStore,
    aiGuideService,
    generatePrompt,
    isAbortError
  ])

  const interruptGeneration = useCallback(() => {
    aiGuideService.interruptCurrentRequest()
    promptStore.setIsTyping(false)
    promptStore.setIsGenerating(false)
    promptStore.setCurrentExecutionStep(null)
  }, [aiGuideService, promptStore])

  return {
    chatContainerMaxHeight,
    initializeChat,
    clearChat,
    sendMessage,
    generatePrompt,
    regenerateMessage,
    resendUserMessage,
    interruptGeneration
  }
}
