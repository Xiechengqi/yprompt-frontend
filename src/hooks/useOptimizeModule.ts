import { useCallback } from 'react'
import { useProviderStore } from '@/stores/providerStore'
import { AIService } from '@/services/aiService'
import { PromptGeneratorService } from '@/services/promptGeneratorService'
import { promptConfigManager } from '@/config/prompts'
import { parseAIJsonResponse } from '@/utils/jsonParser'
import type { PromptAnalysis } from '@/stores/optimizeStore'

export function useOptimizeModule() {
  const providerStore = useProviderStore()
  const aiService = AIService.getInstance()
  const promptGenerator = PromptGeneratorService.getInstance()

  // åˆ†ææç¤ºè¯è´¨é‡
  const analyzePrompt = useCallback(
    async (
      prompt: string,
      _mode: 'system' | 'user',
      onStreamUpdate?: (chunk: string) => void
    ): Promise<PromptAnalysis> => {
      if (!prompt || !prompt.trim()) {
        throw new Error('è¯·è¾“å…¥æç¤ºè¯å†…å®¹')
      }

      const currentProvider = providerStore.currentProvider
      const currentModel = providerStore.currentModel

      if (!currentProvider || !currentModel) {
        throw new Error('è¯·å…ˆé€‰æ‹©AIæä¾›å•†å’Œæ¨¡å‹')
      }

      // ä»é…ç½®ç®¡ç†å™¨è·å–è´¨é‡åˆ†æç³»ç»Ÿæç¤ºè¯
      const systemPrompt = promptConfigManager.getQualityAnalysisSystemPrompt()

      // ç”¨æˆ·æç¤ºè¯ç›´æ¥åœ¨ä»£ç ä¸­æ„å»º
      const userPrompt = `è¯·åˆ†æä»¥ä¸‹ç³»ç»Ÿæç¤ºè¯çš„è´¨é‡ï¼š

æç¤ºè¯å†…å®¹ï¼š
${prompt}

è¯·ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚`

      let fullResponse = ''

      // è®¾ç½®æµå¼å›è°ƒ
      if (onStreamUpdate) {
        aiService.setStreamUpdateCallback((chunk: string) => {
          fullResponse += chunk
          onStreamUpdate(fullResponse)
        })
      }

      try {
        const response = await aiService.callAI(
          [
            { role: 'system', content: systemPrompt },
            { role: 'user', content: userPrompt }
          ],
          currentProvider,
          currentModel.id,
          !!onStreamUpdate // ä½¿ç”¨æµå¼è¾“å‡º
        )

        // ä½¿ç”¨å®Œæ•´å“åº”è¿›è¡Œè§£æ
        const finalContent = response || fullResponse

        // è§£æåˆ†æç»“æœ - å¤„ç†markdownä»£ç å—åŒ…è£¹çš„JSON
        let result: PromptAnalysis
        try {
          result = parseAIJsonResponse(finalContent)
          console.log('âœ… Parsed analysis result:', result)
        } catch (parseError) {
          console.error('âŒ Failed to parse analysis response:', parseError)
          console.error('ğŸ“„ Original content:', finalContent)
          // å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›åŸºç¡€åˆ†æ
          result = {
            overall_score: 75,
            analysis: {
              role: { score: 75, status: 'good', feedback: 'è§’è‰²å®šä¹‰åŸºæœ¬å®Œå–„' },
              task: { score: 75, status: 'good', feedback: 'ä»»åŠ¡æè¿°è¾ƒä¸ºæ¸…æ™°' },
              format: { score: 65, status: 'needs_improvement', feedback: 'è¾“å‡ºæ ¼å¼å¯ä»¥æ›´è¯¦ç»†' },
              constraints: { score: 70, status: 'needs_improvement', feedback: 'çº¦æŸæ¡ä»¶å¯ä»¥æ›´æ˜ç¡®' },
              example: { score: 60, status: 'needs_improvement', feedback: 'å»ºè®®æ·»åŠ ç¤ºä¾‹è¯´æ˜' },
              language: { score: 80, status: 'good', feedback: 'è¯­è¨€è¡¨è¾¾æ¸…æ™°' }
            },
            suggestions: [],
            language: 'zh',
            word_count: prompt.length,
            estimated_tokens: Math.ceil(prompt.length / 4),
            issues: ['è§£æå¤±è´¥ï¼Œæ— æ³•è·å–è¯¦ç»†é—®é¢˜åˆ†æ']
          }
        }

        return result
      } catch (error: any) {
        console.error('Analysis failed:', error)
        // è¿”å›é»˜è®¤åˆ†æç»“æœ
        const defaultResult: PromptAnalysis = {
          overall_score: 70,
          analysis: {
            role: { score: 70, status: 'needs_improvement', feedback: 'åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æç¤ºè¯' },
            task: { score: 70, status: 'needs_improvement', feedback: 'åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æç¤ºè¯' },
            format: { score: 70, status: 'needs_improvement', feedback: 'åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æç¤ºè¯' },
            constraints: { score: 70, status: 'needs_improvement', feedback: 'åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æç¤ºè¯' },
            example: { score: 70, status: 'needs_improvement', feedback: 'åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æç¤ºè¯' },
            language: { score: 70, status: 'needs_improvement', feedback: 'åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æç¤ºè¯' }
          },
          suggestions: [{ area: 'general', suggestion: 'åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é‡è¯•' }],
          language: 'zh',
          word_count: prompt.length,
          estimated_tokens: Math.ceil(prompt.length / 4),
          issues: ['åˆ†æå¤±è´¥']
        }
        return defaultResult
      } finally {
        // æ¸…ç†æµå¼å›è°ƒ
        if (onStreamUpdate) {
          aiService.clearStreamUpdateCallback()
        }
      }
    },
    [providerStore, aiService]
  )

  // ç”Ÿæˆä¼˜åŒ–å»ºè®®
  const generateOptimizationAdvice = useCallback(
    async (
      prompt: string,
      promptType: 'system' | 'user',
      onStreamUpdate?: (chunk: string) => void
    ): Promise<string[]> => {
      const currentProvider = providerStore.currentProvider
      const currentModel = providerStore.currentModel

      if (!currentProvider || !currentModel) {
        throw new Error('è¯·å…ˆé€‰æ‹©AIæä¾›å•†å’Œæ¨¡å‹')
      }

      const adviceList = await promptGenerator.getOptimizationAdvice(
        prompt,
        promptType,
        currentModel.id,
        'zh',
        [],
        currentProvider,
        onStreamUpdate
      )

      return adviceList
    },
    [providerStore, promptGenerator]
  )

  // åº”ç”¨ä¼˜åŒ–å»ºè®®ç”Ÿæˆæœ€ç»ˆæç¤ºè¯
  const applyOptimizationAdvice = useCallback(
    async (
      originalPrompt: string,
      advice: string[],
      promptType: 'system' | 'user',
      onStreamUpdate?: (content: string) => void
    ): Promise<string> => {
      const currentProvider = providerStore.currentProvider
      const currentModel = providerStore.currentModel

      if (!currentProvider || !currentModel) {
        throw new Error('è¯·å…ˆé€‰æ‹©AIæä¾›å•†å’Œæ¨¡å‹')
      }

      const optimizedPrompt = await promptGenerator.applyOptimizationAdvice(
        originalPrompt,
        advice,
        promptType,
        currentModel.id,
        'zh',
        [],
        currentProvider,
        onStreamUpdate
      )

      return optimizedPrompt
    },
    [providerStore, promptGenerator]
  )

  return {
    analyzePrompt,
    generateOptimizationAdvice,
    applyOptimizationAdvice
  }
}
